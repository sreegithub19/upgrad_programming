{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using RNN - Word Level\n",
    "\n",
    "To generate text using RNN, we need a to convert raw text to a supervised learning problem format.\n",
    "\n",
    "Take, for example, the following corpus:\n",
    "\n",
    "\"Her brother shook his head incredulously. He was not aware of the situation at all.\"\n",
    "\n",
    "First we need to divide the data into tabular format containing input (X) and output (y) sequences. In case of a character level model, the X and y will look like this:\n",
    "\n",
    "|      X                |  Y      |\n",
    "|-----------------------|---------|\n",
    "|    < word1 >< word2 > | < word3 > |\n",
    "|    Her brother        |  shook  |\n",
    "|    brother shook      |  his    |\n",
    "|    shook his          |  head   |\n",
    "|    his head           | incredulously |\n",
    "|    head incredulously |    .    |\n",
    "|    ..                 |    .    |\n",
    "|    situation at       |  all    |\n",
    "|    at all             |    .    |\n",
    "\n",
    "Note that in the above problem, the sequence length of **X is two words** and that of **y is one word**. Hence, this is a many-to-one architecture. We can, however, change the number of input words to any number depending on the problem.\n",
    "\n",
    "A model is trained on such data. To generate text, we simply give the model any two words using which it predicts the next word. Then it appends the predicted word to the input sequence (to the extreme right of the sequence) and discards the first word (word on extreme left of the sequence). Then it predicts again using the new sequence and the cycle continues until a fix number of iterations. An example is shown below:\n",
    "\n",
    "Seed text: \"Did I\"\n",
    "\n",
    "|      X                                            |  Y                       |\n",
    "|---------------------------------------------------|--------------------------|\n",
    "|                        Did I                      |    < predicted word 1 >  |\n",
    "|               I < predicted word 1 >              |    < predicted word 2 >  |\n",
    "|       < predicted word 1 > < predicted word 2 >   |    < predicted word 3 >  |\n",
    "|       < predicted word 2 > < predicted word 3 >   |    < predicted word 4 >  |\n",
    "|                      ...                          |            ...           | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "1. Preprocess data\n",
    "2. Build LSTM model\n",
    "3. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "n5R1APadwANc",
    "outputId": "3553d1b6-cb0c-4adf-efac-333da6e7f547",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 04:51:47.324293: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-27 04:51:47.328824: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-27 04:51:47.345297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737953507.366554    2128 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737953507.371108    2128 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-27 04:51:47.395520: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXzkqw1ipoPg"
   },
   "outputs": [],
   "source": [
    "# download ebook\n",
    "url = \"https://www.gutenberg.org/files/24869/24869-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of The Ramayana\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no\n",
      "restrictions whatsoever. You may copy it, give it away or re-use it under\n",
      "the terms of the Project Gutenberg License included with this eBook or\n",
      "online at http://www.gutenberg.org/license\n",
      "\n",
      "\n",
      "\n",
      "Title: The Ramayana\n",
      "\n",
      "\n",
      "\n",
      "Release Date: March 18, 2008 [Ebook #24869]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "\n",
      "***START OF THE PROJECT GUTENBERG EBOOK THE R\n"
     ]
    }
   ],
   "source": [
    "# let's look at the text\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18371\n"
     ]
    }
   ],
   "source": [
    "# subset the book from the first chapter, that ism INVOCATION - everything before first chapter is irrelevant data\n",
    "start_index = re.search(\"invocation.\\(1\\)\", data, re.I)\n",
    "print(start_index.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how does the text look like\n",
    "data = data[start_index.start():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVOCATION.(1)\n",
      "\n",
      "\n",
      "Praise to Válmíki,(2)bird of charming song,(3)\n",
      "  Who mounts on Poesy’s sublimest spray,\n",
      "And sweetly sings with accent clear and strong\n",
      "  Ráma, aye Ráma, in his deathless lay.\n",
      "\n",
      "Where breathes the man can listen to the strain\n",
      "  That flows in music from Válmíki’s tongue,\n",
      "Nor feel his feet the path of bliss attain\n",
      "  When Ráma’s glory by the saint is sung!\n",
      "\n",
      "The stream Rámáyan leaves its sacred fount\n",
      "  The whole wide world from sin and stain to free.(4)\n",
      "The Prince of He\n"
     ]
    }
   ],
   "source": [
    "# let's look at the text\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# define a function to clean text data\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def clean_document(document, char_filter = r\"[^\\w]\"):\n",
    "    '''\n",
    "    input:\n",
    "    document          :  string\n",
    "    char_filter       :  regex pattern - removes those characters from the text that match the pattern\n",
    "\n",
    "    output: clean document\n",
    "    '''\n",
    "    \n",
    "    # convert words to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenise words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # strip whitespace from all words\n",
    "    words = [word.strip() for word in words]\n",
    "\n",
    "    # join back words to get document\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    # remove unwanted characters\n",
    "    document = re.sub(char_filter, \" \", document)\n",
    "\n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    document = re.sub(r\"\\s+\", \" \", document)\n",
    "\n",
    "    # strip whitespace from document\n",
    "    document = document.strip()\n",
    "\n",
    "    return document\n",
    "\n",
    "data = clean_document(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in document: 410371\n"
     ]
    }
   ],
   "source": [
    "# length of text\n",
    "words = word_tokenize(data)\n",
    "print(\"Number of words in document: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PBkfQnPGqV_4",
    "outputId": "5d32cb08-61b0-4695-a4fe-4508b4b3e898"
   },
   "outputs": [],
   "source": [
    "# use Keras' Tokenizer() function to encode text to integers\n",
    "word_tokeniser = Tokenizer()\n",
    "word_tokeniser.fit_on_texts([data])\n",
    "encoded_words = word_tokeniser.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 17667\n"
     ]
    }
   ],
   "source": [
    "# check the size of the vocabulary\n",
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data in X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences\n",
    "\n",
    "In each training sample, X will have a sequence of 5 words and y will have the sixth word. In other words, this means that use previous five words of a sequence to predict next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hwJLfNn3qX56",
    "outputId": "bef2447b-77d1-43f3-e64b-a098ed2e87e1"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "MAX_SEQ_LENGTH = 5  # X will have five words, y will have the sixth word\n",
    "\n",
    "for i in range(MAX_SEQ_LENGTH, len(encoded_words)):\n",
    "    sequence = encoded_words[i-MAX_SEQ_LENGTH:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples: 410241\n",
      "\n",
      "Sample sequences: \n",
      "[[6821  813  857    4 1060 1928]\n",
      " [ 813  857    4 1060 1928  397]\n",
      " [ 857    4 1060 1928  397    3]]\n"
     ]
    }
   ],
   "source": [
    "print('Total number of training samples: {}'.format(len(sequences)))\n",
    "print('\\nSample sequences: \\n{}'.format(sequences[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3baVAiOqZpj"
   },
   "outputs": [],
   "source": [
    "# divide the sequence into X and y\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:,:-1]  # assign all but last words of a sequence to X\n",
    "y = sequences[:,-1]   # assign last word of each sequence to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of the first data point: [6821  813  857    4 1060] \n",
      "\n",
      "Output of the first data point: [ 1928 ]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first training example\n",
    "print(\"Input of the first data point:\", X[0], \"\\n\")\n",
    "print(\"Output of the first data point: [\", y[0], \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410241,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HX_Npp_cVWyW",
    "outputId": "af76e422-420e-4b83-cd42-6b1dda6011ac"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 410241 sequences (data points) in total.\n",
    "\n",
    "Remember that to use an RNN data has to be of the shape (#samples, #timesteps, #features)\n",
    "\n",
    "In X, the third dimension, that is, number of features is missing because we're going to use the Keras' Embedding Layer. Hence we don't need to explicitly reshape the data to incorporate the third dimension. That will be done automatically by Keras.\n",
    "\n",
    "In y, the second dimension is missing, that is, the number of timesteps because y is not a sequence, it's just a single word. The number of features are represented by a one-hot encoded vector whose length is the VOCABULARY_SIZE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "print('Input sequence length: {}'.format(MAX_SEQ_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model architecture\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH))\n",
    "\n",
    "# lstm layer 1\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5134
    },
    "colab_type": "code",
    "id": "cvY3R74rqbOu",
    "outputId": "68541bf7-bc2f-4fa6-c6c9-8787a7da8e51"
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "model.fit(X, y, epochs=10, verbose=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embeddings to represent the input words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec download link (Size ~ 1.5GB): https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "path = 'storage/word-embeddings/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# load word2vec using the following function present in the gensim library\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign word vectors from word2vec model\n",
    "\n",
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokeniser.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check embedding dimension\n",
    "print(\"Embeddings shape: {}\".format(embedding_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model architecture\n",
    "\n",
    "model_wv = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model_wv.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH,\n",
    "                    weights = [embedding_weights], trainable=True))\n",
    "\n",
    "# lstm layer 1\n",
    "model_wv.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "# when using multiple LSTM layers, set return_sequences to True at the previous layer\n",
    "# because the current layer expects a sequential intput rather than a single input\n",
    "model_wv.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model_wv.add(Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model_wv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model_wv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5134
    },
    "colab_type": "code",
    "id": "cvY3R74rqbOu",
    "outputId": "68541bf7-bc2f-4fa6-c6c9-8787a7da8e51"
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "model_wv.fit(X, y, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NiH8h5oqkX5"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed, n_words):\n",
    "    \n",
    "    text = seed\n",
    "    \n",
    "    # generate n_words\n",
    "    for _ in range(n_words):\n",
    "        \n",
    "        # encode text as integers\n",
    "        encoded_words = word_tokeniser.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # pad sequences\n",
    "        padded_words = pad_sequences([encoded_words], maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "        \n",
    "        # predict next word\n",
    "        prediction = model.predict_classes(padded_words, verbose=0)\n",
    "        \n",
    "        # convert predicted index to its word\n",
    "        next_word = \"\"\n",
    "        for word, i in word_tokeniser.word_index.items():\n",
    "            if i == prediction:\n",
    "                next_word = word\n",
    "                break\n",
    "        \n",
    "        # append predicted word to text\n",
    "        text += \" \" + next_word\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some text generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using first model - model without word embeddings\n",
    "seed_text = \"rama never told anyone about\"\n",
    "num_words = 100\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using second model - model with word embeddings\n",
    "seed_text = \"rama never told anyone about\"\n",
    "num_words = 100\n",
    "print(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using first model - model without word embeddings\n",
    "seed_text = \"how are you doing\"\n",
    "num_words = 100\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using second model - model with word embeddings\n",
    "seed_text = \"how are you doing\"\n",
    "num_words = 100\n",
    "print(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "word_rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
